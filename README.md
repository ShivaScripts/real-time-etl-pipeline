# ğŸš€ Realâ€‘Time ETL Pipeline  
*A realâ€‘time ETL pipeline leveraging Kafka, Spark, Airflow, and Docker containers.*  

---

## ğŸ“‹ Summary  
This pipeline automatically generates example log messages, sends them through Kafka, saves the raw logs, cleans and analyzes them with Spark, and is scheduled and monitored by Airflow.
- ğŸ”„ **Streaming Ingestion:** Apache Kafka ingests synthetic logs in real time.  
- ğŸ¤– **Data Generation:** Python `Faker` library creates realistic fake log entries.  
- âš¡ **Stream Processing:** Spark Structured Streaming transforms and aggregates logs.  
- ğŸ“† **Workflow Orchestration:** Airflow schedules & monitors your ETL DAG.  
- ğŸ³ **Containerized Deployment:** Docker Compose spins up Kafka, Spark, Airflow, etc.

---
## Architecture Diagram

![Architecture](assets/architecture.png)

---

## Demo / Screenshots

Below are key screenshots demonstrating the pipeline in action.

### 1. Airflow DAG Running
![Airflow DAG Working](assets/airflow.png)
*Airflow UI showing the `log_file_processor` DAG with tasks `run_producer`, `run_consumer`, and `process_raw_logs` all succeeding.*

---

### 2. Cleaned Data by Spark
![Spark Cleaned Data](assets/sparkc.png)
*Directory listing of cleaned JSON output under `data/processed_logs/cleaned`, produced by the Spark Structured Streaming job.*

---

### 3. Aggregated Metrics by Spark
![Spark Metrics](assets/sparkm.png)
*Directory listing of Parquet files under `data/processed_logs/metrics`, containing the 1-minute window aggregations.*

---

### 4. Raw Logs Stored
![Raw Logs](assets/raw.png)
*Directory listing of raw JSON log batches under `data/raw_logs`, showing files created by the Kafka consumer.*

---

## âœ¨ Features  
- **Real-Time Data Ingestion** via Kafka topics  
- **Synthetic Log Generation** with `Faker`  
- **Fault-Tolerant Stream Processing** using Spark (exactly-once semantics)  
- **DAG-Based Scheduling** through Airflow  
- **Oneâ€‘Click Deployment** with Docker Compose  

---

## ğŸ›  Tech Stack  
- **Kafka** â€“ Distributed streaming platform  
- **Spark Structured Streaming** â€“ Scalable, fault-tolerant stream processing  
- **Airflow** â€“ Workflow orchestration and scheduling  
- **Docker & Docker Compose** â€“ Containerization and multi-service orchestration  
- **Python** â€“ Core language for scripts  
- **Faker** â€“ Synthetic data generation library  

---

## ğŸ—‚ï¸ Folder Structure  
real_time_data_pipeline/
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml         # (optional) for containerized Kafka/Spark/Airflow
â”œâ”€â”€ docker/                    # Dockerfiles, if any
â”‚   â””â”€â”€ python-app/
â”‚       â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ src/                       # Application source code
â”‚   â”œâ”€â”€ producer/
â”‚   â”‚   â””â”€â”€ producer.py        # Kafka producer: generates synthetic logs via Faker
â”‚   â”œâ”€â”€ consumer/
â”‚   â”‚   â””â”€â”€ consumer.py        # Kafka consumer: writes raw JSON to data/raw_logs/
â”‚   â””â”€â”€ sparkk/
â”‚       â””â”€â”€ spark_streaming_consumer.py  # Spark Structured Streaming job
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ log_file_processor_dag.py    # Airflow DAG orchestrating tasks
â”‚   â”œâ”€â”€ webserver_config.py             # (optional) custom Airflow settings
â”‚   â””â”€â”€ logs/                           # Airflow runtime logs
â”‚
â”œâ”€â”€ data/                       # Local data storage (or mounted volumes)
â”‚   â”œâ”€â”€ raw_logs/               # Raw JSON batches created by consumer
â”‚   â””â”€â”€ processed_logs/
â”‚       â”œâ”€â”€ cleaned/            # Cleaned JSON output by Spark
â”‚       â””â”€â”€ metrics/            # Parquet metrics by Spark
â”‚
â”œâ”€â”€ assets/                     # Documentation assets
â”‚   â”œâ”€â”€ architecture.png        # Architecture diagram
â”‚   â”œâ”€â”€ airflow.png             # Airflow DAG screenshot
â”‚   â”œâ”€â”€ sparkc.png              # Spark cleaned data screenshot
â”‚   â”œâ”€â”€ sparkm.png              # Spark metrics screenshot
â”‚   â””â”€â”€ raw.png                 # Raw logs screenshot
â”‚
â””â”€â”€ README.md                   # This file

